{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a1883f-4b69-4342-8aa0-4e0b97dd788d",
   "metadata": {
    "id": "f3a1883f-4b69-4342-8aa0-4e0b97dd788d"
   },
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a5aac-095a-4049-86ea-2eb6695ed35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:27:29.717974Z",
     "iopub.status.busy": "2024-11-25T18:27:29.717415Z",
     "iopub.status.idle": "2024-11-25T18:27:31.228431Z",
     "shell.execute_reply": "2024-11-25T18:27:31.227830Z",
     "shell.execute_reply.started": "2024-11-25T18:27:29.717953Z"
    },
    "id": "d01a5aac-095a-4049-86ea-2eb6695ed35d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.autograd import grad\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771f6ec-13b1-4017-8dc8-7d1c984ef57d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:27:59.530899Z",
     "iopub.status.busy": "2024-11-25T18:27:59.530091Z",
     "iopub.status.idle": "2024-11-25T18:28:27.184497Z",
     "shell.execute_reply": "2024-11-25T18:28:27.184060Z",
     "shell.execute_reply.started": "2024-11-25T18:27:59.530866Z"
    },
    "id": "f771f6ec-13b1-4017-8dc8-7d1c984ef57d"
   },
   "outputs": [],
   "source": [
    "# Load the updated interaction array and cluster labels\n",
    "interaction_array = np.load('normalized_interaction_array.npy').astype('float32')  #  array with BERT embeddings\n",
    "cluster_labels = np.load('user_cluster_labels_with_embeddings.npy').astype('int')  #  cluster labels\n",
    "trainX = torch.tensor(np.load('trainX.npy'))\n",
    "trainY = torch.tensor(np.load('trainY.npy'))\n",
    "testX = torch.tensor(np.load('testX.npy'))\n",
    "testY = torch.tensor(np.load('testY.npy'))\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(trainX, trainY)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# Parameters\n",
    "latent_dim = 200\n",
    "num_classes = 26  # Number of clusters\n",
    "output_size = interaction_array.shape[1]  # Adjusted for the new interaction array dimensions\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06bdef-14ac-40fc-aaea-ce957efafe38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:28:57.266065Z",
     "iopub.status.busy": "2024-11-25T18:28:57.265159Z",
     "iopub.status.idle": "2024-11-25T18:28:57.723240Z",
     "shell.execute_reply": "2024-11-25T18:28:57.722791Z",
     "shell.execute_reply.started": "2024-11-25T18:28:57.266049Z"
    },
    "id": "8d06bdef-14ac-40fc-aaea-ce957efafe38"
   },
   "outputs": [],
   "source": [
    "# 1. Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, 128)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 128, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(2048),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "\n",
    "            nn.Linear(1024, output_size),\n",
    "            nn.Tanh()#\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        input_data = torch.cat((noise, label_embed), dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "\n",
    "# 2. Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_size, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, 128)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_size + 128, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        input_data = torch.cat((data, label_embed), dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim, num_classes, output_size).to(device)\n",
    "discriminator = Discriminator(output_size, num_classes).to(device)\n",
    "\n",
    "torch.save(generator, 'teacher_generator_model.pth')\n",
    "torch.save(discriminator, 'teacher_discriminator_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c01045-8777-401d-89fb-0937ce367ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:14:55.222983Z",
     "iopub.status.busy": "2024-11-25T18:14:55.222486Z"
    },
    "id": "c4c01045-8777-401d-89fb-0937ce367ae7",
    "outputId": "750c4998-8f69-483b-e93e-e24710c3168b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/750, D Loss: 0.2391, G Loss: -0.9373, D Accuracy: 54.20%, G Accuracy: 93.70%\n",
      "Best discriminator saved at epoch 3 with accuracy: 54.20%\n",
      "Best generator saved at epoch 3 with accuracy: 93.70%\n",
      "Checkpoint saved at epoch 3\n",
      "Epoch 4/750, D Loss: -0.0673, G Loss: -0.9527, D Accuracy: 53.75%, G Accuracy: 90.60%\n",
      "Checkpoint saved at epoch 4\n",
      "Epoch 5/750, D Loss: -0.0859, G Loss: -0.9526, D Accuracy: 53.80%, G Accuracy: 92.10%\n",
      "Checkpoint saved at epoch 5\n",
      "Epoch 6/750, D Loss: 0.0579, G Loss: -0.9062, D Accuracy: 53.65%, G Accuracy: 92.20%\n",
      "Checkpoint saved at epoch 6\n",
      "Epoch 7/750, D Loss: -0.1376, G Loss: -0.8906, D Accuracy: 54.45%, G Accuracy: 92.00%\n",
      "Best discriminator saved at epoch 7 with accuracy: 54.45%\n",
      "Checkpoint saved at epoch 7\n",
      "Epoch 8/750, D Loss: -0.2236, G Loss: -0.9062, D Accuracy: 54.20%, G Accuracy: 91.50%\n",
      "Checkpoint saved at epoch 8\n",
      "Epoch 9/750, D Loss: -0.2338, G Loss: -0.9372, D Accuracy: 53.75%, G Accuracy: 93.20%\n",
      "Checkpoint saved at epoch 9\n",
      "Epoch 10/750, D Loss: 0.0011, G Loss: -0.9218, D Accuracy: 54.25%, G Accuracy: 92.00%\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch 11/750, D Loss: 0.0720, G Loss: -0.6367, D Accuracy: 53.75%, G Accuracy: 91.70%\n",
      "Checkpoint saved at epoch 11\n",
      "Epoch 12/750, D Loss: -0.4516, G Loss: -0.9217, D Accuracy: 54.15%, G Accuracy: 91.60%\n",
      "Checkpoint saved at epoch 12\n",
      "Epoch 13/750, D Loss: -0.2015, G Loss: -0.8906, D Accuracy: 53.45%, G Accuracy: 93.10%\n",
      "Checkpoint saved at epoch 13\n",
      "Epoch 14/750, D Loss: -0.2407, G Loss: -0.9372, D Accuracy: 53.75%, G Accuracy: 92.90%\n",
      "Checkpoint saved at epoch 14\n",
      "Epoch 15/750, D Loss: -0.4807, G Loss: -0.9218, D Accuracy: 54.05%, G Accuracy: 93.40%\n",
      "Checkpoint saved at epoch 15\n",
      "Epoch 16/750, D Loss: -0.3591, G Loss: -0.9524, D Accuracy: 54.15%, G Accuracy: 93.00%\n",
      "Checkpoint saved at epoch 16\n",
      "Epoch 17/750, D Loss: 0.7423, G Loss: -0.9218, D Accuracy: 53.85%, G Accuracy: 92.90%\n",
      "Checkpoint saved at epoch 17\n",
      "Epoch 18/750, D Loss: -0.5626, G Loss: -0.9526, D Accuracy: 54.45%, G Accuracy: 90.30%\n",
      "Checkpoint saved at epoch 18\n",
      "Epoch 19/750, D Loss: -0.3774, G Loss: -0.9524, D Accuracy: 53.45%, G Accuracy: 92.30%\n",
      "Checkpoint saved at epoch 19\n",
      "Epoch 20/750, D Loss: -0.2518, G Loss: -0.9621, D Accuracy: 53.30%, G Accuracy: 91.50%\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch 21/750, D Loss: -0.3189, G Loss: -0.8750, D Accuracy: 53.35%, G Accuracy: 90.80%\n",
      "Checkpoint saved at epoch 21\n",
      "Epoch 22/750, D Loss: -0.0446, G Loss: -0.9062, D Accuracy: 53.60%, G Accuracy: 92.00%\n",
      "Checkpoint saved at epoch 22\n",
      "Epoch 23/750, D Loss: -0.2453, G Loss: -0.9374, D Accuracy: 53.75%, G Accuracy: 91.60%\n",
      "Checkpoint saved at epoch 23\n",
      "Epoch 24/750, D Loss: -0.3905, G Loss: -0.9370, D Accuracy: 53.60%, G Accuracy: 93.00%\n",
      "Checkpoint saved at epoch 24\n",
      "Epoch 25/750, D Loss: 0.2147, G Loss: -0.8905, D Accuracy: 53.85%, G Accuracy: 92.20%\n",
      "Checkpoint saved at epoch 25\n",
      "Epoch 26/750, D Loss: -0.1346, G Loss: -0.8750, D Accuracy: 53.75%, G Accuracy: 91.10%\n",
      "Checkpoint saved at epoch 26\n",
      "Epoch 27/750, D Loss: -0.0836, G Loss: -0.9373, D Accuracy: 53.85%, G Accuracy: 92.70%\n",
      "Checkpoint saved at epoch 27\n",
      "Epoch 28/750, D Loss: -0.1164, G Loss: -0.8906, D Accuracy: 53.90%, G Accuracy: 92.10%\n",
      "Checkpoint saved at epoch 28\n",
      "Epoch 29/750, D Loss: -0.1667, G Loss: -0.9218, D Accuracy: 53.00%, G Accuracy: 91.30%\n",
      "Checkpoint saved at epoch 29\n",
      "Epoch 30/750, D Loss: -0.2596, G Loss: -0.9373, D Accuracy: 53.45%, G Accuracy: 92.50%\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch 31/750, D Loss: -0.2064, G Loss: -0.8906, D Accuracy: 53.95%, G Accuracy: 91.50%\n",
      "Checkpoint saved at epoch 31\n",
      "Epoch 32/750, D Loss: -0.3902, G Loss: -0.9062, D Accuracy: 52.90%, G Accuracy: 92.80%\n",
      "Checkpoint saved at epoch 32\n",
      "Epoch 33/750, D Loss: -0.0643, G Loss: -0.8594, D Accuracy: 53.15%, G Accuracy: 92.20%\n",
      "Checkpoint saved at epoch 33\n",
      "Epoch 34/750, D Loss: -0.2753, G Loss: -0.9527, D Accuracy: 54.45%, G Accuracy: 92.60%\n",
      "Checkpoint saved at epoch 34\n",
      "Epoch 35/750, D Loss: 0.1471, G Loss: -0.9217, D Accuracy: 54.70%, G Accuracy: 92.40%\n",
      "Best discriminator saved at epoch 35 with accuracy: 54.70%\n",
      "Checkpoint saved at epoch 35\n",
      "Epoch 36/750, D Loss: -0.1747, G Loss: -0.9372, D Accuracy: 53.80%, G Accuracy: 92.00%\n",
      "Checkpoint saved at epoch 36\n",
      "Epoch 37/750, D Loss: -0.2616, G Loss: -0.9217, D Accuracy: 53.70%, G Accuracy: 93.50%\n",
      "Checkpoint saved at epoch 37\n",
      "Epoch 38/750, D Loss: -0.2731, G Loss: -0.9062, D Accuracy: 54.35%, G Accuracy: 92.80%\n",
      "Checkpoint saved at epoch 38\n",
      "Epoch 39/750, D Loss: -0.1914, G Loss: -0.9524, D Accuracy: 53.65%, G Accuracy: 91.90%\n",
      "Checkpoint saved at epoch 39\n",
      "Epoch 40/750, D Loss: -0.4953, G Loss: -0.8906, D Accuracy: 53.75%, G Accuracy: 92.70%\n",
      "Checkpoint saved at epoch 40\n",
      "Epoch 41/750, D Loss: 0.4923, G Loss: -0.9373, D Accuracy: 54.20%, G Accuracy: 93.10%\n",
      "Checkpoint saved at epoch 41\n",
      "Epoch 42/750, D Loss: -0.3222, G Loss: -0.9215, D Accuracy: 53.65%, G Accuracy: 91.30%\n",
      "Checkpoint saved at epoch 42\n",
      "Epoch 43/750, D Loss: -0.0660, G Loss: -0.9665, D Accuracy: 53.60%, G Accuracy: 92.20%\n",
      "Checkpoint saved at epoch 43\n",
      "Epoch 44/750, D Loss: -0.5437, G Loss: -0.9217, D Accuracy: 54.30%, G Accuracy: 92.40%\n",
      "Checkpoint saved at epoch 44\n",
      "Epoch 45/750, D Loss: -0.3676, G Loss: -0.9373, D Accuracy: 53.80%, G Accuracy: 90.70%\n",
      "Checkpoint saved at epoch 45\n",
      "Epoch 46/750, D Loss: -0.5301, G Loss: -0.9527, D Accuracy: 54.60%, G Accuracy: 91.60%\n",
      "Checkpoint saved at epoch 46\n",
      "Epoch 47/750, D Loss: -0.2711, G Loss: -0.8594, D Accuracy: 53.80%, G Accuracy: 92.20%\n",
      "Checkpoint saved at epoch 47\n",
      "Epoch 48/750, D Loss: 0.3735, G Loss: -0.8905, D Accuracy: 53.50%, G Accuracy: 91.70%\n",
      "Checkpoint saved at epoch 48\n",
      "Epoch 49/750, D Loss: 0.1742, G Loss: -0.9668, D Accuracy: 54.20%, G Accuracy: 93.20%\n",
      "Checkpoint saved at epoch 49\n",
      "Epoch 50/750, D Loss: -0.2491, G Loss: -0.8437, D Accuracy: 53.50%, G Accuracy: 92.30%\n",
      "Checkpoint saved at epoch 50\n",
      "Epoch 51/750, D Loss: -0.5259, G Loss: -0.9218, D Accuracy: 53.25%, G Accuracy: 91.60%\n",
      "Checkpoint saved at epoch 51\n",
      "Epoch 52/750, D Loss: -0.2317, G Loss: -0.9676, D Accuracy: 53.65%, G Accuracy: 91.40%\n",
      "Checkpoint saved at epoch 52\n",
      "Epoch 53/750, D Loss: -0.3736, G Loss: -0.9374, D Accuracy: 53.80%, G Accuracy: 92.90%\n",
      "Checkpoint saved at epoch 53\n",
      "Epoch 54/750, D Loss: -0.4120, G Loss: -0.9218, D Accuracy: 54.20%, G Accuracy: 92.90%\n",
      "Checkpoint saved at epoch 54\n",
      "Epoch 55/750, D Loss: -0.2382, G Loss: -0.9678, D Accuracy: 54.30%, G Accuracy: 91.50%\n",
      "Checkpoint saved at epoch 55\n",
      "Epoch 56/750, D Loss: -0.4129, G Loss: -0.8906, D Accuracy: 54.45%, G Accuracy: 91.90%\n",
      "Checkpoint saved at epoch 56\n",
      "Epoch 57/750, D Loss: -0.2078, G Loss: -0.9218, D Accuracy: 54.60%, G Accuracy: 92.40%\n",
      "Checkpoint saved at epoch 57\n"
     ]
    }
   ],
   "source": [
    "# Train CGAN with Balanced Training Strategy\n",
    "def train_cgan(generator, discriminator, train_loader, latent_dim, num_classes, epochs=750, checkpoint_path='teacher_cgan_checkpoint_new.pth'):\n",
    "    best_g_loss = float('inf')\n",
    "    best_d_accuracy = 0.0\n",
    "    best_g_accuracy = 0.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Adjust discriminator and generator training frequency\n",
    "    k_d = 1  # Train discriminator every k_d epochs\n",
    "    k_g = 2  # Train generator k_g times per epoch\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_epoch, best_g_loss = load_checkpoint(generator, discriminator, optimizer_g, optimizer_d, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch + 1, epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for real_data, real_labels in train_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "            real_data, real_labels = real_data.to(device), real_labels.to(device)\n",
    "\n",
    "            # ----------------------\n",
    "            # Train Discriminator\n",
    "            # ----------------------\n",
    "            if epoch % k_d == 0:  # Train discriminator less frequently\n",
    "                optimizer_d.zero_grad()\n",
    "\n",
    "                # Real data\n",
    "                real_target = torch.full((batch_size, 1), 0.9, device=device)  # Label smoothing for real data\n",
    "                d_real = discriminator(real_data, real_labels)\n",
    "\n",
    "                # Fake data\n",
    "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                fake_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "                fake_data = generator(noise, fake_labels)\n",
    "                fake_target = torch.full((batch_size, 1), -0.9, device=device)  # Label smoothing for fake data\n",
    "                d_fake = discriminator(fake_data.detach(), fake_labels)\n",
    "\n",
    "                # Total discriminator loss\n",
    "                d_loss = d_loss_real + d_loss_fake + 5 * gp  # Gradient penalty weight reduced to 5\n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            # ----------------------\n",
    "            # Train Generator\n",
    "            # ----------------------\n",
    "            for _ in range(k_g):  # Train generator more frequently\n",
    "                optimizer_g.zero_grad()\n",
    "\n",
    "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                fake_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "                fake_data = generator(noise, fake_labels)\n",
    "\n",
    "                # Generator tries to fool the discriminator\n",
    "                g_loss = -torch.mean(discriminator(fake_data, fake_labels))  # Negative Wasserstein loss\n",
    "                g_loss.backward()\n",
    "                optimizer_g.step()\n",
    "\n",
    "\n",
    "train_cgan(generator, discriminator, train_loader, latent_dim, num_classes, epochs=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b7c3a-c004-4120-9447-cde1e39e1a3d",
   "metadata": {
    "id": "6c9b7c3a-c004-4120-9447-cde1e39e1a3d"
   },
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daf2f8-7883-4741-a8ea-707cc449129d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T18:50:29.969046Z",
     "iopub.status.busy": "2024-11-25T18:50:29.968433Z",
     "iopub.status.idle": "2024-11-25T18:50:29.976460Z",
     "shell.execute_reply": "2024-11-25T18:50:29.975984Z",
     "shell.execute_reply.started": "2024-11-25T18:50:29.969013Z"
    },
    "id": "f2daf2f8-7883-4741-a8ea-707cc449129d"
   },
   "outputs": [],
   "source": [
    "class StudentGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, output_size):\n",
    "        super(StudentGenerator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, 60)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 60, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        input_data = torch.cat((noise, label_embed), dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "\n",
    "class StudentDiscriminator(nn.Module):\n",
    "    def __init__(self, output_size, num_classes):\n",
    "        super(StudentDiscriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, 60)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(output_size + 60, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        input_data = torch.cat((data, label_embed), dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "\n",
    "# Student Models\n",
    "student_generator = StudentGenerator(latent_dim, num_classes, output_size).to(device)\n",
    "student_discriminator = StudentDiscriminator(output_size, num_classes).to(device)\n",
    "\n",
    "torch.save(student_generator, 'student_generator_model.pth')\n",
    "torch.save(student_discriminator, 'student_discriminator_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c6d4b-5625-47c0-bfc3-e67a15e8998a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T19:00:57.957507Z",
     "iopub.status.busy": "2024-11-25T19:00:57.957209Z",
     "iopub.status.idle": "2024-11-25T19:00:58.338983Z",
     "shell.execute_reply": "2024-11-25T19:00:58.337051Z",
     "shell.execute_reply.started": "2024-11-25T19:00:57.957447Z"
    },
    "id": "df2c6d4b-5625-47c0-bfc3-e67a15e8998a",
    "outputId": "1493c94a-a902-48e7-84de-c2579362796b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting from scratch.\n",
      "Epoch 1/500, D Loss: 2.0070, G Loss: -0.4669, D Accuracy: 61.00%, G Accuracy: 76.40%\n",
      "Best discriminator saved at epoch 1 with accuracy: 61.00%\n",
      "Best generator saved at epoch 1 with accuracy: 76.40%\n",
      "Checkpoint saved at epoch 1\n",
      "Epoch 2/500, D Loss: 1.3682, G Loss: -0.5340, D Accuracy: 68.35%, G Accuracy: 71.10%\n",
      "Best discriminator saved at epoch 2 with accuracy: 68.35%\n",
      "Reloading best generator at epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FYP-1\\AppData\\Local\\Temp\\ipykernel_11000\\4256877323.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_generator.load_state_dict(torch.load(\"best_student_generator.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 2\n",
      "Epoch 3/500, D Loss: 2.4447, G Loss: -0.4897, D Accuracy: 67.75%, G Accuracy: 65.50%\n",
      "Reloading best generator at epoch 3...\n",
      "Checkpoint saved at epoch 3\n",
      "Epoch 4/500, D Loss: 2.6902, G Loss: -0.5748, D Accuracy: 66.45%, G Accuracy: 52.20%\n",
      "Reloading best generator at epoch 4...\n",
      "Checkpoint saved at epoch 4\n",
      "Epoch 5/500, D Loss: 2.2835, G Loss: -0.5513, D Accuracy: 79.20%, G Accuracy: 39.90%\n",
      "Best discriminator saved at epoch 5 with accuracy: 79.20%\n",
      "Reloading best generator at epoch 5...\n",
      "Checkpoint saved at epoch 5\n",
      "Epoch 6/500, D Loss: 4.5508, G Loss: -0.3765, D Accuracy: 86.80%, G Accuracy: 31.10%\n",
      "Best discriminator saved at epoch 6 with accuracy: 86.80%\n",
      "Reloading best generator at epoch 6...\n",
      "Checkpoint saved at epoch 6\n",
      "Epoch 7/500, D Loss: 2.8791, G Loss: -0.4757, D Accuracy: 70.60%, G Accuracy: 44.30%\n",
      "Reloading best discriminator at epoch 7...\n",
      "Reloading best generator at epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FYP-1\\AppData\\Local\\Temp\\ipykernel_11000\\4256877323.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_discriminator.load_state_dict(torch.load(\"best_student_discriminator.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 7\n",
      "Epoch 8/500, D Loss: 3.1936, G Loss: -0.3251, D Accuracy: 83.75%, G Accuracy: 35.50%\n",
      "Reloading best generator at epoch 8...\n",
      "Checkpoint saved at epoch 8\n",
      "Epoch 9/500, D Loss: 2.2916, G Loss: -0.6019, D Accuracy: 5.90%, G Accuracy: 87.60%\n",
      "Best generator saved at epoch 9 with accuracy: 87.60%\n",
      "Reloading best discriminator at epoch 9...\n",
      "Checkpoint saved at epoch 9\n",
      "Epoch 10/500, D Loss: 2.6960, G Loss: -0.6553, D Accuracy: 61.10%, G Accuracy: 70.30%\n",
      "Reloading best discriminator at epoch 10...\n",
      "Reloading best generator at epoch 10...\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch 11/500, D Loss: 2.4469, G Loss: -0.6423, D Accuracy: 6.40%, G Accuracy: 88.10%\n",
      "Best generator saved at epoch 11 with accuracy: 88.10%\n",
      "Reloading best discriminator at epoch 11...\n",
      "Checkpoint saved at epoch 11\n",
      "Epoch 12/500, D Loss: 3.2216, G Loss: -0.7126, D Accuracy: 55.80%, G Accuracy: 90.10%\n",
      "Best generator saved at epoch 12 with accuracy: 90.10%\n",
      "Reloading best discriminator at epoch 12...\n",
      "Checkpoint saved at epoch 12\n",
      "Epoch 13/500, D Loss: 3.5427, G Loss: -0.7142, D Accuracy: 5.45%, G Accuracy: 89.20%\n",
      "Reloading best discriminator at epoch 13...\n",
      "Checkpoint saved at epoch 13\n",
      "Epoch 14/500, D Loss: 2.2144, G Loss: -0.7276, D Accuracy: 45.85%, G Accuracy: 90.20%\n",
      "Best generator saved at epoch 14 with accuracy: 90.20%\n",
      "Reloading best discriminator at epoch 14...\n",
      "Checkpoint saved at epoch 14\n",
      "Epoch 15/500, D Loss: 2.6288, G Loss: -0.7589, D Accuracy: 55.55%, G Accuracy: 87.90%\n",
      "Reloading best discriminator at epoch 15...\n",
      "Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 132\u001b[0m\n\u001b[0;32m    120\u001b[0m         save_student_checkpoint(\n\u001b[0;32m    121\u001b[0m             epoch,\n\u001b[0;32m    122\u001b[0m             student_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m             checkpoint_path,\n\u001b[0;32m    129\u001b[0m         )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m train_student_cgan(teacher_generator, teacher_discriminator, student_generator, student_discriminator, train_loader, latent_dim, num_classes, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 44\u001b[0m, in \u001b[0;36mtrain_student_cgan\u001b[1;34m(teacher_generator, teacher_discriminator, student_generator, student_discriminator, train_loader, latent_dim, num_classes, epochs, checkpoint_path)\u001b[0m\n\u001b[0;32m     42\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     43\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, num_classes, (batch_size,), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 44\u001b[0m student_fake_data \u001b[38;5;241m=\u001b[39m student_generator(noise, fake_labels)\n\u001b[0;32m     45\u001b[0m d_fake \u001b[38;5;241m=\u001b[39m student_discriminator(student_fake_data\u001b[38;5;241m.\u001b[39mdetach(), fake_labels)\n\u001b[0;32m     46\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(d_fake)\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mStudentGenerator.forward\u001b[1;34m(self, noise, labels)\u001b[0m\n\u001b[0;32m     20\u001b[0m label_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_embedding(labels)\n\u001b[0;32m     21\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((noise, label_embed), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_data)\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FYP-1\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_student_cgan(teacher_generator,teacher_discriminator,student_generator,student_discriminator,train_loader,latent_dim,num_classes, epochs=500,checkpoint_path=\"student_cgan_checkpoint.pth\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_d_accuracy = 0.0\n",
    "    best_g_accuracy = 0.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Adjust training frequencies for student models\n",
    "    k_d = 1  # Train discriminator every k_d epochs\n",
    "    k_g = 2  # Train generator k_g times per epoch\n",
    "\n",
    "    # Optimizers for the student models\n",
    "    optimizer_sg = optim.Adam(student_generator.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
    "    optimizer_sd = optim.Adam(student_discriminator.parameters(), lr=0.00002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Loss function for distillation\n",
    "    distillation_loss = nn.MSELoss()\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_epoch, best_d_accuracy, best_g_accuracy = load_student_checkpoint(student_generator, student_discriminator, optimizer_sg, optimizer_sd, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch + 1, epochs):\n",
    "        student_generator.train()\n",
    "        student_discriminator.train()\n",
    "\n",
    "        for real_data, real_labels in train_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "            real_data, real_labels = real_data.to(device), real_labels.to(device)\n",
    "\n",
    "            # ----------------------\n",
    "            # Train Student Discriminator\n",
    "            # ----------------------\n",
    "            if epoch % k_d == 0:  # Train discriminator less frequently\n",
    "                optimizer_sd.zero_grad()\n",
    "\n",
    "                # Real data\n",
    "                d_real = student_discriminator(real_data, real_labels)\n",
    "                d_loss_real = -torch.mean(d_real)\n",
    "\n",
    "                # Fake data\n",
    "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                fake_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "                student_fake_data = student_generator(noise, fake_labels)\n",
    "                d_fake = student_discriminator(student_fake_data.detach(), fake_labels)\n",
    "                d_loss_fake = torch.mean(d_fake)\n",
    "\n",
    "                # Gradient penalty\n",
    "                gp = compute_gradient_penalty(student_discriminator, real_data, student_fake_data.detach(), real_labels)\n",
    "\n",
    "                # Total discriminator loss\n",
    "                d_loss = d_loss_real + d_loss_fake + 10 * gp  # Gradient penalty weight\n",
    "                d_loss.backward()\n",
    "                optimizer_sd.step()\n",
    "\n",
    "            # ----------------------\n",
    "            # Train Student Generator\n",
    "            # ----------------------\n",
    "           # Train Student Generator\n",
    "            for _ in range(k_g):  # Train generator more frequently\n",
    "                optimizer_sg.zero_grad()\n",
    "\n",
    "                # Recompute fake data\n",
    "                noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "                fake_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "                student_fake_data = student_generator(noise, fake_labels)\n",
    "\n",
    "                # Generator adversarial loss\n",
    "                student_g_fake = student_discriminator(student_fake_data, fake_labels)\n",
    "                g_loss_adv = -torch.mean(student_g_fake)\n",
    "\n",
    "\n",
    "\n",
    "train_student_cgan(teacher_generator, teacher_discriminator, student_generator, student_discriminator, train_loader, latent_dim, num_classes, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2548e-daa2-4924-b107-ecfbed5e6213",
   "metadata": {
    "id": "63e2548e-daa2-4924-b107-ecfbed5e6213"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
